# OpenAI API configuration (for vLLM server compatibility)
# Set OPENAI_API_KEY to a placeholder or actual key if your vLLM server requires it for authentication,
# even if it's a local server. If your local vLLM server doesn't require an API key, you can leave it blank.

# =============================================================================
# General access tokens
# =============================================================================

# Tavily API key (if you plan to use Tavily for research)
TAVILY_API_KEY=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

# Github Personal Access Token (if using github mcp server)
# GITHUB_PERSONAL_ACCESS_TOKEN=your_github_token

# =============================================================================
# Python Script LLM configuration (used to parse the insights into JSON)
# =============================================================================
OPENAI_API_BASE=http://192.168.8.90:42069/v1
OPENAI_API_KEY=outsider
OPENAI_MODEL_NAME=gpt-oss-120b
#OPENAI_MODEL_NAME=qwen3-think-30b-awq8
#OPENAI_MODEL_NAME=glm-45-air
#OPENAI_MODEL_NAME=qwen3-next-80b-inst
#OPENAI_MODEL_NAME=qwen3-next-80b-think
#OPENAI_MODEL_NAME=qwen3-vl-30b-inst

# =============================================================================
# GPTresearcher configuration (using local vllm)
# =============================================================================

VLLM_OPENAI_API_BASE=http://192.168.8.90:42069/v1
VLLM_OPENAI_API_KEY=outsider
#VLLM_OPENAI_API_BASE=https://openrouter.ai/api/v1
#VLLM_OPENAI_API_KEY=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

FAST_LLM=vllm_openai:gpt-oss-120b
#FAST_LLM=vllm_openai:qwen3-next-80b-inst
#FAST_LLM=vllm_openai:qwen3-next-80b-think
#FAST_LLM=vllm_openai:glm-45-air
#FAST_LLM=vllm_openai:qwen3-vl-30b-inst

#FAST_LLM=vllm_openai:gpt-5-nano
---
SMART_LLM=vllm_openai:gpt-oss-120b
#SMART_LLM=vllm_openai:qwen3-next-80b-inst
#SMART_LLM=vllm_openai:qwen3-next-80b-think
#SMART_LLM=vllm_openai:glm-45-air
#SMART_LLM=vllm_openai:qwen3-vl-30b-inst

#SMART_LLM=vllm_openai:gpt-5-nano
---
STRATEGIC_LLM=vllm_openai:gpt-oss-120b
#STRATEGIC_LLM=vllm_openai:qwen3-next-80b-inst
#STRATEGIC_LLM=vllm_openai:qwen3-next-80b-think
#STRATEGIC_LLM=vllm_openai:glm-45-air
#STRATEGIC_LLM=vllm_openai:qwen3-vl-30b-inst

#STRATEGIC_LLM=vllm_openai:gpt-5-nano
---
TEMPERATURE=0.7

# GPT 5 cost
LLM_IN_PRICE_PER_1K=.000125 
LLM_OUT_PRICE_PER_1K=.01

# oss-120b on open router cost
#LLM_IN_PRICE_PER_1K=.000125 
#LLM_OUT_PRICE_PER_1K=.01

# =============================================================================
# EMBEDDING CONFIGURATION - Choose ONE option below
# =============================================================================

# OPTION 1: Ollama Server on LAN (Currently Active)
# Fast, runs on dedicated server, good for distributed setups

OLLAMA_BASE_URL=http://192.168.8.24:11434
#EMBEDDING=ollama:mxbai-embed-large
EMBEDDING=ollama:embeddinggemma
#EMBEDDING=ollama:qwen3-embedding

# OPTION 2: Local HuggingFace Sentence Transformers (Backup)
# Slower, runs locally, good for single-machine setups
# EMBEDDING=huggingface:sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# SEARCH ENGINE CONFIGURATION - Choose ONE option below
# =============================================================================

# SearXNG URL for self-hosted search engine
SEARX_URL=https://search.roci.me/
#SEARX_URL=http://192.168.8.24:8095/
#SEARCH_PROVIDER=tavily  # Options: tavily, serpapi, google

# OPTION 1: SearXNG + MCP Hybrid (Default)
# Privacy-first self-hosted search with AI reasoning
#RETRIEVER=searx,mcp

# OPTION 2: Tavily + MCP Hybrid (Backup)
# Commercial search API with AI reasoning
#RETRIEVER=tavily,mcp

# OPTION 3: SearXNG Only
# Pure self-hosted search, no AI reasoning
#RETRIEVER=searx

# OPTION 4: Tavily Only
# Pure commercial search API, no AI reasoning
RETRIEVER=tavily

# =============================================================================
# Web Scraping Configuration
# =============================================================================

SCRAPER=bs  # Options: bs (BeautifulSoup), browser (Selenium), tavily_extract, firecrawl

# =============================================================================
# RESEARCH DEPTH / WEBSITE LIMIT SETTINGS (add or adjust as needed)
# =============================================================================
#MAX_SEARCH_RESULTS_PER_QUERY=2           # How many URLs to fetch per query | Default:5, typical range: 1-20
#MAX_ITERATIONS=4                         # Number of refinement loops (depth) | Default:3, typical range: 1-5 (6 for very deep runs)
#DEEP_RESEARCH_BREADTH=7                  # Parallel research paths | Default:4, typical range: 1-8
#DEEP_RESEARCH_DEPTH=3                    # Levels of recursion per path | Default:2, typical range: 1-4
#DEEP_RESEARCH_CONCURRENCY=4              # Async concurrency for deep research | Default:4, typical range: 1-16
#TOTAL_WORDS=4500                         # Target length of final report | Default: 1200, typical range: any
#SIMILARITY_THRESHOLD=0.5                 # Filter low‑relevance docs | Default:0.42, typical range: 0.2-0.8
#MAX_SUBTOPICS=5                          # Max sub‑topics generated | Default:3, typical range: 1-10

#BROWSE_CHUNK_MAX_LENGTH=32768
#FAST_TOKEN_LIMIT=16000                   # Anything over 16000 causes errors and no tokens are generated
#SMART_TOKEN_LIMIT=16000                  # Anything over 16000 causes errors and no tokens are generated

#CURATE_SOURCES=true                      # Whether to curate sources for research. This step adds an LLM run which may increase costs and total run time but improves quality of source selection. Defaults to False.
#REASONING_EFFORT=high                    # Controls the reasoning effort of strategic models. Default to medium.

#  
MAX_SEARCH_RESULTS_PER_QUERY=100          # How many URLs to fetch per query | Default:5, typical range: 1-20
MAX_ITERATIONS=4                         # Number of refinement loops (depth) | Default:3, typical range: 1-5 (6 for very deep runs)
DEEP_RESEARCH_BREADTH=7                  # Parallel research paths | Default:4, typical range: 1-8
DEEP_RESEARCH_DEPTH=3                    # Levels of recursion per path | Default:2, typical range: 1-4
DEEP_RESEARCH_CONCURRENCY=4              # Async concurrency for deep research | Default:4, typical range: 1-16
TOTAL_WORDS=4500                         # Target length of final report | Default: 1200, typical range: any
SIMILARITY_THRESHOLD=0.5                 # Filter low‑relevance docs | Default:0.42, typical range: 0.2-0.8
MAX_SUBTOPICS=10                          # Max sub‑topics generated | Default:3, typical range: 1-10

BROWSE_CHUNK_MAX_LENGTH=32768
FAST_TOKEN_LIMIT=16000                   # Anything over 16000 causes errors and no tokens are generated
SMART_TOKEN_LIMIT=16000                  # Anything over 16000 causes errors and no tokens are generated

CURATE_SOURCES=true                      # Whether to curate sources for research. This step adds an LLM run which may increase costs and total run time but improves quality of source selection. Defaults to False.
REASONING_EFFORT=high                    # Controls the reasoning effort of strategic models. Default to medium.


# Test scripts variables
OPENROUTER_API_KEY=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