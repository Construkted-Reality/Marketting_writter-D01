#!/usr/bin/env python3
"""
Simple script to generate marketing blog posts using a local OpenAI compliant API endpoint.
Based on the reference implementation pattern from cr_content_pipeline.py
Uses built-in system and user prompts for Construkted Reality marketing content.
"""

import argparse
import json
import os
import re
import sys
import time
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv
from openai import OpenAI

def load_environment() -> None:
    """Load .env file and set environment variables."""
    load_dotenv()
    
    # Validate required OpenAI configuration
    openai_api_base = os.getenv("OPENAI_API_BASE")
    if not openai_api_base:
        raise ValueError("OPENAI_API_BASE environment variable is not set.")
    os.environ["OPENAI_API_BASE"] = openai_api_base
    
    # Set API key (use dummy key if server doesn't require authentication)
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY", "sk-dummy-key-if-not-needed")
    
    # Validate model name
    openai_model_name = os.getenv("OPENAI_MODEL_NAME")
    if not openai_model_name:
        raise ValueError("OPENAI_MODEL_NAME environment variable is not set.")
    os.environ["OPENAI_MODEL_NAME"] = openai_model_name


def read_reference_file(file_path: str) -> str:
    """Read and return the content of a reference file."""
    try:
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"Reference file not found: {file_path}")
        if not path.is_file():
            raise ValueError(f"Path is not a file: {file_path}")
        return path.read_text(encoding="utf-8")
    except Exception as e:
        raise RuntimeError(f"Failed to read reference file {file_path}: {e}")


def build_system_prompt() -> str:
    """
    Build the system prompt by combining content from reference files.
    
    Returns:
        The complete system prompt string
    """
    try:
        # Read the three reference context files
        writing_style_content = read_reference_file("reference_context/writing_style-enhanced.md")
        market_analysis_content = read_reference_file("reference_context/Combined_Small_Team_Geospatial_Market_Analysis.md")
        construkted_context_content = read_reference_file("reference_context/construkted_context.md")
        
        # Build the system prompt by combining all content
        system_prompt = f"""You are a masterful marketing copywriter for the company Construkted Reality. You generate engaging blog articles using the style guide provided.

WRITING STYLE GUIDE:
{writing_style_content}

COMPANY CONTEXT:
{construkted_context_content}

MARKET RESEARCH CONTEXT:
{market_analysis_content}

When writing marketing content, always:
1. Follow the writing style guidelines precisely
2. Incorporate company context and mission naturally
3. Reference market insights where relevant to strengthen arguments
4. Maintain an engaging, conversational tone that educates while exciting
5. Focus on the benefits of user-generated 3D data and community collaboration
6. Avoid corporate jargon and speak directly to both professionals and hobbyists"""
        
        return system_prompt
        
    except Exception as e:
        raise RuntimeError(f"Failed to build system prompt: {e}")


def filter_think_tags(text: str) -> str:
    """
    Filter out content between <think> and </think> tags.
    
    Args:
        text: The text to filter
        
    Returns:
        The filtered text with think tag content removed
    """
    # Pattern to match content between <think> and </think> tags
    pattern = r'<think>.*?</think>'
    # Use DOTALL flag to match across newlines
    filtered_text = re.sub(pattern, '', text, flags=re.DOTALL)
    # Clean up any extra whitespace that might be left
    filtered_text = re.sub(r'\n\s*\n', '\n\n', filtered_text)
    return filtered_text.strip()


def build_user_prompt() -> str:
    """
    Build the user prompt for the Construkted Globe blog post.
    
    Returns:
        The user prompt string
    """
    return """Given the company context and the market research in the GIS space for small business, please write a marketing blog post article talking about the Construkted Globe generated by the public 3d scan data contributed by the users.

Talk about the current state and where it is going. I also need this blog post to encourage users to upload data to help grow the user generated Construkted globe. Talk about the benefits of having a user generated 3d digital globe which is NOT controlled by large corporations (like Google earth, or Bing Maps, and other large corporate overlords doing the mapping).

There are no alterior motives other then to have a user generated dataset with no restrictions. (no restrictions other than the fact that the contributed 3d scans must be of real places)"""

def send_to_llm(
    user_prompt: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 4000,
    retry_count: int = 3,
    retry_delay: float = 1.0,
    verbose: bool = False
) -> str:
    """
    Send prompt to the local OpenAI compliant API endpoint.
    
    Args:
        user_prompt: User prompt/instruction
        system_prompt: System prompt (uses built-in prompt if None)
        temperature: Sampling temperature (default 0.7)
        max_tokens: Maximum tokens to generate (default 4000)
        retry_count: Number of API call retries (default 3)
        retry_delay: Delay between retries in seconds (default 1.0)
        verbose: Enable verbose logging (default False)
        
    Returns:
        The LLM response content as a string
    """
    # Create OpenAI client using the configured vLLM endpoint
    client = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY", "sk-dummy-key-if-not-needed"),
        base_url=os.getenv("OPENAI_API_BASE")
    )
    
    # Use built-in system prompt if none provided
    if system_prompt is None:
        system_prompt = build_system_prompt()
    
    # Prepare messages for the API
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    # Get model name from environment
    model_name = os.getenv("OPENAI_MODEL_NAME")
    if not model_name:
        raise ValueError("OPENAI_MODEL_NAME environment variable is not set")
    
    last_error = None
    for attempt in range(retry_count):
        try:
            # Make API call to the local OpenAI compliant endpoint
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,  # type: ignore[arg-type]
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # Extract and return the response content
            response_content = response.choices[0].message.content
            if response_content is None:
                raise ValueError("Empty response from LLM")
                
            return response_content.strip()
            
        except Exception as e:
            last_error = e
            if verbose:
                print(f"API call attempt {attempt + 1} failed: {e}")
            
            if attempt < retry_count - 1:  # Don't sleep on the last attempt
                if verbose:
                    print(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
    
    # If we get here, all retries failed
    raise RuntimeError(f"Failed to get response from LLM after {retry_count} attempts. Last error: {last_error}")


def main():
    """Main function to handle command line arguments and execute the script."""
    parser = argparse.ArgumentParser(
        description="Generate marketing blog posts using a local OpenAI compliant API endpoint."
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Sampling temperature (default: 0.7)"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=4000,
        help="Maximum tokens to generate (default: 4000)"
    )
    parser.add_argument(
        "--output",
        help="Optional file path to save the LLM response"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose output"
    )
    parser.add_argument(
        "--retry-count",
        type=int,
        default=3,
        help="Number of API call retries (default: 3)"
    )
    parser.add_argument(
        "--retry-delay",
        type=float,
        default=1.0,
        help="Delay between retries in seconds (default: 1.0)"
    )
    parser.add_argument(
        "--filter-think",
        action="store_true",
        help="Filter out content between <think> tags from the LLM response"
    )
    
    args = parser.parse_args()
    
    try:
        # Load environment variables
        if args.verbose:
            print("Loading environment variables...")
        load_environment()
        
        if args.verbose:
            print(f"API Base URL: {os.getenv('OPENAI_API_BASE')}")
            print(f"Model Name: {os.getenv('OPENAI_MODEL_NAME')}")
            print("Using built-in system and user prompts for Construkted Reality marketing content")
        
        # Build prompts
        system_prompt = build_system_prompt()
        user_prompt = build_user_prompt()
        
        if args.verbose:
            print("Built system and user prompts from reference context files")
        
        # Send to LLM
        if args.verbose:
            print("Sending request to LLM...")
        
        response = send_to_llm(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            retry_count=args.retry_count,
            retry_delay=args.retry_delay,
            verbose=args.verbose
        )
        
        # Apply think tag filtering if requested
        if args.filter_think:
            if args.verbose:
                print("Filtering out content between <think> tags...")
            response = filter_think_tags(response)
        
        # Output the response
        print("=" * 80)
        print("LLM RESPONSE:")
        print("=" * 80)
        print(response)
        print("=" * 80)
        
        # Save to file if requested
        if args.output:
            output_path = Path(args.output)
            output_path.write_text(response, encoding="utf-8")
            if args.verbose:
                print(f"Response saved to: {output_path}")
        
        return 0
        
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())